# Полное Описание Проекта (Адаптировано под Готовый Датасет)

## 1. Обзор Проекта

**Назначение:** Это full-stack веб-приложение, предназначенное для обнаружения аномалий в данных (например, имитирующих деятельность интернет-магазина или другую активность). Оно включает запись пользовательской активности (если применимо к датасету), применение различных алгоритмов машинного обучения для выявления подозрительных паттернов и предоставление пользовательского интерфейса для визуализации данных и результатов анализа на основе **готового датасета**.

**Основные цели:**
*   Демонстрация применения различных методов обнаружения аномалий (статистических, на основе плотности, ансамблевых, нейросетевых) на предоставленном датасете.
*   Предоставление интерактивного инструмента для анализа данных и исследования аномалий.
*   Создание расширяемой платформы для добавления новых источников данных и детекторов.

## 2. Архитектура

Проект разделен на две основные части:

*   **Frontend:** Одностраничное приложение (SPA), написанное на React + Vite, отвечающее за пользовательский интерфейс, отображение данных и взаимодействие с пользователем.
*   **Backend:** API-сервер, написанный на Python с использованием FastAPI, который обрабатывает запросы от фронтенда, взаимодействует с базой данных (куда загружен или где обрабатывается датасет), и выполняет задачи машинного обучения.

**Взаимодействие:** Frontend общается с Backend через RESTful API.

## 3. Frontend (`frontend/`)

*   **Технологии:**
    *   React
    *   Vite (сборщик)
    *   Material UI (MUI) или Chakra UI (библиотека компонентов)
    *   Chart.js & react-chartjs-2 (графики)
    *   Axios (HTTP-запросы)
    *   React Router (навигация)
    *   Date-fns (работа с датами)
*   **Основные Разделы (Страницы):**
    *   `/` (Панель Управления): Сводная информация, недавние аномалии, основные графики по датасету.
    *   `/customers`, `/products`, `/orders`, `/activities` (или аналогичные сущности из датасета): Просмотр списков сущностей.
    *   `/analysis` (Анализ Признаков): Визуализация данных с помощью Scatter Plot (выбор сущности и признаков, подсветка аномалий).
    *   `/explanation` (О Методах): Описание ML-алгоритмов.
    *   `/settings` (Настройки): Конфигурация параметров ML.
    *   *(Возможно)* `/dataset` (Управление Датасетом): Интерфейс для загрузки/выбора/просмотра информации о датасете (если требуется).
*   **Запуск:** `npm install` -> `npm run dev` (в директории `frontend`)

## 4. Backend (`backend/`)

*   **Технологии:**
    *   FastAPI (веб-фреймворк)
    *   Uvicorn (ASGI сервер)
    *   SQLAlchemy (ORM для работы с БД)
    *   Pydantic (валидация данных API)
    *   SQLite (или другая БД для хранения/обработки датасета и результатов)
    *   Scikit-learn, Pandas, NumPy (анализ данных и ML)
    *   TensorFlow/Keras (для модели Autoencoder) или PyTorch (желательно)
    *   Joblib (сохранение/загрузка ML-моделей/объектов)
    *   Python-dotenv (управление переменными окружения)
*   **Структура Директории:**
    *   `main.py`: Точка входа FastAPI приложения, инициализация, CORS, подключение роутеров.
    *   `database.py`: Настройка SQLAlchemy (engine, SessionLocal, Base, get_db). Читает `DATABASE_URL` из `.env`.
    *   `.env`: Файл переменных окружения (содержит `DATABASE_URL`).
    *   `models.py`: Определения моделей данных SQLAlchemy (ORM-классы таблиц для сущностей датасета и аномалий).
    *   `schemas.py`: Определения схем Pydantic для валидации API запросов/ответов.
    *   `crud.py`: Функции для выполнения операций CRUD с базой данных (чтение данных датасета, запись аномалий).
    *   `api/`: Директория с роутерами FastAPI, группирующими эндпоинты по логике:
        *   `data_source.py` (или по имени сущности): Эндпоинты для доступа к данным датасета (товары, клиенты, активности и т.д.).
        *   `anomalies.py`: Обучение, детекция, получение аномалий.
        *   `dashboard.py`: Сводная статистика.
        *   `charts.py`: Данные для графиков.
        *   `settings.py`: Управление настройками ML.
        *   *(Возможно)* `dataset_management.py`: Эндпоинты для управления датасетом (загрузка, выбор).
    *   `ml_service/`: Модуль для логики машинного обучения.
        *   `common.py`: Общие функции (feature engineering), константы (признаки, пути).
        *   `detector.py`: Реализация детекторов (Statistical Z-score, Isolation Forest, DBSCAN).
        *   `autoencoder_detector.py` (или `pytorch_detector.py`): Реализация нейросетевого детектора.
        *   `saved_models/`: Директория для сохранения обученных моделей, скейлеров, статистик.
    *   `utils.py`: Вспомогательные функции.
    *   `requirements.txt`: Список зависимостей Python.
    *   *(Возможно)* `scripts/`: Скрипты для загрузки/предобработки исходного датасета в БД.
*   **Запуск:** `pip install -r backend/requirements.txt` -> `uvicorn backend.main:app --reload --port 8001` (в корневой директории проекта, с активным виртуальным окружением).

### 4.1. Структура Базы Данных (`backend/models.py`)

База данных (например, SQLite) содержит таблицы, соответствующие структуре **используемого датасета** (например, `activities`, `transactions`, `logs` и т.д.), а также таблицы для хранения результатов:

*   **Таблицы Датасета:** (Например, `UserActivity`, `Order`, `Product`, `Customer` - структура зависит от конкретного датасета).
*   **`anomalies` (Модель `Anomaly`)**
    *   `id`: Уникальный идентификатор записи об аномалии.
    *   `entity_type`: Тип сущности из датасета, в которой найдена аномалия.
    *   `entity_id`: ID аномальной записи в таблице датасета.
    *   `detection_timestamp`: Время обнаружения аномалии.
    *   `description`: Сгенерированное описание аномалии.
    *   `severity`: Оценка серьезности ('low', 'medium', 'high').
    *   `detector_name`: Название детектора.
    *   `details`: Дополнительные детали от детектора (JSON).
    *   `anomaly_score`: Численная оценка аномальности.
*   **`settings` (Модель `Setting`)**
    *   `key`: Уникальное имя настройки ML.
    *   `value`: Значение настройки.

### 4.2. Краткий Обзор API Эндпоинтов (`backend/api/`)

*   `/api/data`: CRUD операции или эндпоинты для чтения сущностей из датасета (с пагинацией/фильтрацией).
*   `/api/anomalies`: Запуск обучения (`POST /train`), запуск детекции (`POST /detect`), получение списка обнаруженных аномалий (`GET /`), удаление аномалий (`DELETE /`).
*   `/api/dashboard`: Получение сводной статистики (`GET /summary`).
*   `/api/charts`: Получение данных для графиков на фронтенде.
*   `/api/settings`: Получение (`GET /`) и обновление (`PUT /`) настроек ML.
*   *(Возможно)* `/api/dataset`: Эндпоинты для управления датасетом.

Полная интерактивная документация доступна через Swagger UI по адресу `/docs` после запуска бэкенда.

## 5. Сервис Машинного Обучения (`backend/ml_service/`)

Отвечает за весь пайплайн обнаружения аномалий в данных из предоставленного датасета.

### 5.1. Инженерия Признаков (`common.engineer_features`)

*   **Цель:** Преобразовать данные из датасета в числовые и категориальные признаки, пригодные для моделей.
*   **Процесс:** Зависит от структуры датасета. Может включать:
    1.  Создание DataFrame из записей датасета.
    2.  Извлечение/расчет признаков (например, временных, статистических, категориальных).
    3.  Обработка пропусков.
    4.  Очистка типов данных.
*   **Основные Признаки:** Определяются на основе анализа датасета.

### 5.2. Предобработка

*   **One-Hot Encoding (OHE):** Для категориальных признаков.
*   **Масштабирование (`StandardScaler` или др.):** Для числовых признаков (важно для некоторых алгоритмов).

### 5.3. Детекторы Аномалий

*   **Statistical Z-Score (`StatisticalDetector`)**
*   **Isolation Forest (`IsolationForestDetector`)**
*   **DBSCAN (`DbscanDetector`)**
*   **Autoencoder/PyTorch Model (`NeuralNetworkDetector`)**
    *   (Принцип, Обучение, Детекция адаптированы под конкретную нейросетевую архитектуру)

### 5.4. Сохранение и Загрузка

*   Обученные модели, статистики, объекты предобработки сохраняются в `backend/ml_service/saved_models/`.

## 6. Примерный Рабочий Процесс (Обнаружение Аномалии)

1.  **Подготовка Данных:** Готовый датасет загружается в базу данных (или доступен иным способом для Backend).
2.  **(По запросу из UI)** Пользователь запускает детекцию аномалий через Frontend (`POST /api/anomalies/detect`).
3.  **Backend (`api/anomalies.py`):**
    *   Получает параметры (тип сущности, список алгоритмов, параметры датасета).
    *   Вызывает `crud` функции для чтения нужных данных из датасета.
    *   Вызывает `ml_service.common.engineer_features` для расчета признаков.
    *   Для каждого выбранного алгоритма:
        *   Создает экземпляр детектора.
        *   Вызывает метод `.detect()` детектора, передавая признаки.
        *   Детектор выполняет предобработку, применяет модель, определяет аномальные точки (записи из датасета).
    *   Для каждой найденной аномальной записи:
        *   Вызывает `crud.create_anomaly`, сохраняя информацию об аномалии в таблице `anomalies`.
4.  **Frontend** запрашивает список аномалий (`GET /api/anomalies`).
5.  **Backend (`api/anomalies.py`)** вызывает `crud.get_anomalies`, получает данные из таблицы `anomalies`.
6.  **Frontend** отображает список аномалий пользователю.

## 7. Заключение

Этот проект представляет собой комплексное решение для демонстрации и исследования методов обнаружения аномалий на **предоставленном датасете**. Сочетание современного веб-стека (React, FastAPI) с разнообразными ML-алгоритмами позволяет гибко анализировать данные и визуализировать результаты. 